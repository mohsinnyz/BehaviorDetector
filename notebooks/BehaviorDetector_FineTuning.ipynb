{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ğŸš€ BEHAVIOR DETECTOR - TRAINING SCRIPT\n",
        "# ==========================================\n",
        "!pip install -q ultralytics roboflow\n",
        "import os, shutil, yaml, random\n",
        "from roboflow import Roboflow\n",
        "from glob import glob\n",
        "from ultralytics import YOLO\n"
      ],
      "metadata": {
        "id": "JiuoKbx4dpXY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. CONFIG\n",
        "# ==========================================\n",
        "API_KEY = \"YPl8sjuu0ikszmZbb3yH\"\n",
        "\n",
        "PHONE_DS_ID    = \"learning-mmxpm/phone-detection-tqu3x\"\n",
        "FOOD_DS_ID     = \"smartlight/eating-person-models\"\n",
        "DRINK_DS_ID    = \"ai-bt7mn/drinking-huzef\"\n",
        "PERSONAL_DS_ID = \"student-b3qgz/behaviordetector\"\n",
        "\n",
        "MAX_IMAGES = 200\n",
        "FINAL_DIR = \"/content/merged_dataset\"\n",
        "random.seed(42)\n",
        "\n",
        "TARGET_CLASSES = {\n",
        "    0: ['phone', 'mobile', 'cell', 'smart'],\n",
        "    1: ['food', 'eat', 'burger', 'sandwich'],\n",
        "    2: ['drink', 'cup', 'bottle', 'water']\n",
        "}"
      ],
      "metadata": {
        "id": "ZFD4PLSBkdOr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. DOWNLOAD & PREPARE\n",
        "# ==========================================\n",
        "rf = Roboflow(api_key=API_KEY)\n",
        "\n",
        "# Setup folders\n",
        "for split in ['train', 'valid']:\n",
        "    os.makedirs(os.path.join(FINAL_DIR, split, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(FINAL_DIR, split, 'labels'), exist_ok=True)\n",
        "\n",
        "def get_dataset(dataset_id):\n",
        "    \"\"\"Downloads dataset and returns the path.\"\"\"\n",
        "    try:\n",
        "        print(f\"â¬‡ï¸ Downloading {dataset_id}...\")\n",
        "        parts = dataset_id.split(\"/\")\n",
        "        project = rf.workspace(parts[0]).project(parts[1])\n",
        "        # Download version 1 (or latest available)\n",
        "        dataset = project.version(project.versions()[0].version).download(\"yolov8\")\n",
        "        return dataset.location\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error downloading {dataset_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "path_phone = get_dataset(PHONE_DS_ID)\n",
        "path_food = get_dataset(FOOD_DS_ID)\n",
        "path_drink = get_dataset(DRINK_DS_ID)\n",
        "path_personal = get_dataset(PERSONAL_DS_ID)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xujrArTid8N3",
        "outputId": "51dad650-2162-44bd-cfd3-f8cd3bd03b77"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸ Downloading learning-mmxpm/phone-detection-tqu3x...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "â¬‡ï¸ Downloading smartlight/eating-person-models...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "â¬‡ï¸ Downloading ai-bt7mn/drinking-huzef...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "â¬‡ï¸ Downloading student-b3qgz/behaviordetector...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. PROCESS & MERGE (FULL FIXED VERSION)\n",
        "# ==========================================\n",
        "def normalize(name):\n",
        "    return name.lower().replace(\"-\", \"\").replace(\"_\", \"\").replace(\" \", \"\")\n",
        "\n",
        "def process_dataset(source_path, target_class, prefix, limit=None):\n",
        "    if not source_path:\n",
        "        return 0\n",
        "\n",
        "    # Load class names\n",
        "    with open(f\"{source_path}/data.yaml\") as f:\n",
        "        names = yaml.safe_load(f)[\"names\"]\n",
        "\n",
        "    # =============================\n",
        "    # Detect numeric class names\n",
        "    # =============================\n",
        "    if all(n.isdigit() for n in names):\n",
        "        print(f\"   âš¡ Numeric class names detected in {source_path}, forcing mapping for target {target_class}\")\n",
        "        source_id = 0  # phone dataset is class 0\n",
        "    else:\n",
        "        # keyword-based matching for string names\n",
        "        source_id = None\n",
        "        for i, name in enumerate(names):\n",
        "            norm_name = normalize(name)\n",
        "            for kw in TARGET_CLASSES[target_class]:\n",
        "                if kw in norm_name:\n",
        "                    source_id = i\n",
        "                    break\n",
        "\n",
        "    if source_id is None:\n",
        "        print(f\"âš ï¸ No matching class found in {source_path}\")\n",
        "        print(f\"   Available classes: {names}\")\n",
        "        return 0\n",
        "    else:\n",
        "        print(f\"   â„¹ï¸ Mapping '{names[source_id]}' â†’ target {target_class}\")\n",
        "\n",
        "    # =============================\n",
        "    # Process images\n",
        "    # =============================\n",
        "    images = glob(f\"{source_path}/train/images/*\")\n",
        "    random.shuffle(images)\n",
        "\n",
        "    count = 0\n",
        "    for img in images:\n",
        "        if limit and count >= limit:\n",
        "            break\n",
        "\n",
        "        base = os.path.splitext(os.path.basename(img))[0]\n",
        "        lbl_path = f\"{source_path}/train/labels/{base}.txt\"\n",
        "        if not os.path.exists(lbl_path):\n",
        "            continue\n",
        "\n",
        "        with open(lbl_path) as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        new_lines = []\n",
        "        for line in lines:\n",
        "            cls, *bbox = line.split()\n",
        "            if int(cls) == source_id:\n",
        "                new_lines.append(f\"{target_class} {' '.join(bbox)}\\n\")\n",
        "\n",
        "        if not new_lines:\n",
        "            continue\n",
        "\n",
        "        new_img = f\"{prefix}_{base}.jpg\"\n",
        "        new_lbl = f\"{prefix}_{base}.txt\"\n",
        "\n",
        "        shutil.copy(img, f\"{FINAL_DIR}/train/images/{new_img}\")\n",
        "        with open(f\"{FINAL_DIR}/train/labels/{new_lbl}\", \"w\") as f:\n",
        "            f.writelines(new_lines)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    print(f\"âœ… Added {count} images from {prefix}\")\n",
        "    return count\n",
        "\n",
        "# ==========================================\n",
        "# RUN MERGE FOR PUBLIC DATASETS\n",
        "# ==========================================\n",
        "print(\"\\nâš™ï¸ Merging datasets...\")\n",
        "process_dataset(paths[\"phone\"], 0, \"phone\", MAX_IMAGES)\n",
        "process_dataset(paths[\"food\"], 1, \"food\", MAX_IMAGES)\n",
        "process_dataset(paths[\"drink\"], 2, \"drink\", MAX_IMAGES)\n",
        "\n",
        "# ==========================================\n",
        "# PERSONAL DATA (SAFE REMAP)\n",
        "# ==========================================\n",
        "if paths[\"personal\"]:\n",
        "    print(\"âš™ï¸ Processing personal dataset\")\n",
        "\n",
        "    with open(f\"{paths['personal']}/data.yaml\") as f:\n",
        "        p_names = yaml.safe_load(f)[\"names\"]\n",
        "\n",
        "    # Normalize personal dataset names\n",
        "    def normalize_personal(name):\n",
        "        return name.lower().replace(\"-\", \"\").replace(\"_\", \"\").replace(\" \", \"\")\n",
        "\n",
        "    name_to_target = {normalize_personal(name): i for i, name in enumerate(['phone', 'food', 'drink'])}\n",
        "\n",
        "    imgs = glob(f\"{paths['personal']}/train/images/*\")\n",
        "    for img in imgs:\n",
        "        base = os.path.splitext(os.path.basename(img))[0]\n",
        "        lbl = f\"{paths['personal']}/train/labels/{base}.txt\"\n",
        "        if not os.path.exists(lbl):\n",
        "            continue\n",
        "\n",
        "        with open(lbl) as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        new_lines = []\n",
        "        for line in lines:\n",
        "            cls, *bbox = line.split()\n",
        "            cname = normalize_personal(p_names[int(cls)])\n",
        "            if cname in name_to_target:\n",
        "                new_lines.append(f\"{name_to_target[cname]} {' '.join(bbox)}\\n\")\n",
        "\n",
        "        if not new_lines:\n",
        "            continue\n",
        "\n",
        "        shutil.copy(img, f\"{FINAL_DIR}/train/images/personal_{base}.jpg\")\n",
        "        with open(f\"{FINAL_DIR}/train/labels/personal_{base}.txt\", \"w\") as f:\n",
        "            f.writelines(new_lines)\n",
        "\n",
        "    print(\"âœ… Personal data merged\")\n"
      ],
      "metadata": {
        "id": "1QqeqHRHe5yc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa07cc7-968f-4252-8f9a-4ddcb9e4ed1a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âš™ï¸ Merging datasets...\n",
            "   âš¡ Numeric class names detected in /content/Phone-Detection-1, forcing mapping for target 0\n",
            "   â„¹ï¸ Mapping '0' â†’ target 0\n",
            "âœ… Added 200 images from phone\n",
            "   â„¹ï¸ Mapping 'eating-person' â†’ target 1\n",
            "âœ… Added 200 images from food\n",
            "   â„¹ï¸ Mapping 'drink-water' â†’ target 2\n",
            "âœ… Added 200 images from drink\n",
            "âš™ï¸ Processing personal dataset\n",
            "âœ… Personal data merged\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. MERGE & REMAP LOGIC\n",
        "# ==========================================\n",
        "# Target Classes: 0=phone, 1=food, 2=drink\n",
        "\n",
        "def process_dataset(source_path, target_class_id, limit=200):\n",
        "    if not source_path: return\n",
        "\n",
        "    # Inspect data.yaml to find the source class ID\n",
        "    with open(os.path.join(source_path, \"data.yaml\"), 'r') as f:\n",
        "        data = yaml.safe_load(f)\n",
        "        names = data['names']\n",
        "\n",
        "    # Auto-find the best matching class ID in the source\n",
        "    # (e.g., if source has ['head', 'phone'], we want ID 1)\n",
        "    source_id = 0\n",
        "    keywords = {\n",
        "        0: ['phone', 'mobile', 'cell', 'smart'], # Target 0 (Phone)\n",
        "        1: ['food', 'eat', 'burger', 'sandwich'], # Target 1 (Food)\n",
        "        2: ['drink', 'cup', 'bottle', 'water']    # Target 2 (Drink)\n",
        "    }\n",
        "\n",
        "    # Simple search for the right ID\n",
        "    for idx, name in enumerate(names):\n",
        "        if any(k in name.lower() for k in keywords[target_class_id]):\n",
        "            source_id = idx\n",
        "            break\n",
        "\n",
        "    print(f\"   â„¹ï¸ Mapping '{names[source_id]}' (ID {source_id}) -> Target ID {target_class_id}\")\n",
        "\n",
        "    # Process Images\n",
        "    images = glob(os.path.join(source_path, \"train\", \"images\", \"*\"))\n",
        "    random.shuffle(images)\n",
        "\n",
        "    count = 0\n",
        "    for img_path in images:\n",
        "        if count >= limit: break\n",
        "\n",
        "        # Find corresponding label\n",
        "        basename = os.path.basename(img_path)\n",
        "        name_only = os.path.splitext(basename)[0]\n",
        "        label_file = os.path.join(source_path, \"train\", \"labels\", name_only + \".txt\")\n",
        "\n",
        "        if not os.path.exists(label_file): continue\n",
        "\n",
        "        # Read label file\n",
        "        with open(label_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        new_lines = []\n",
        "        has_item = False\n",
        "        for line in lines:\n",
        "            parts = line.split()\n",
        "            cls = int(parts[0])\n",
        "\n",
        "            # If this line matches our desired object, remap and save\n",
        "            if cls == source_id:\n",
        "                new_line = f\"{target_class_id} \" + \" \".join(parts[1:]) + \"\\n\"\n",
        "                new_lines.append(new_line)\n",
        "                has_item = True\n",
        "\n",
        "        if has_item:\n",
        "            # Copy image\n",
        "            shutil.copy(img_path, os.path.join(FINAL_DIR, \"train\", \"images\", basename))\n",
        "            # Write new label\n",
        "            with open(os.path.join(FINAL_DIR, \"train\", \"labels\", name_only + \".txt\"), \"w\") as f:\n",
        "                f.writelines(new_lines)\n",
        "            count += 1\n",
        "\n",
        "    print(f\"   âœ… Added {count} images.\")\n",
        "\n",
        "print(\"\\nâš™ï¸ Processing Data...\")\n",
        "# Process Public Data (200 each)\n",
        "process_dataset(path_phone, target_class_id=0, limit=MAX_IMAGES)\n",
        "process_dataset(path_food, target_class_id=1, limit=MAX_IMAGES)\n",
        "process_dataset(path_drink, target_class_id=2, limit=MAX_IMAGES)\n",
        "\n",
        "# Process PERSONAL Data (Take ALL of it, no limit)\n",
        "if path_personal:\n",
        "    print(\"   â„¹ï¸ Adding Personal Data (All)...\")\n",
        "    # Assuming personal data is already 0=phone, 1=food, 2=drink\n",
        "    # If uncertain, we could map it, but usually personal data is the gold standard.\n",
        "    p_imgs = glob(os.path.join(path_personal, \"train\", \"images\", \"*\"))\n",
        "    for img in p_imgs:\n",
        "        shutil.copy(img, os.path.join(FINAL_DIR, \"train\", \"images\", os.path.basename(img)))\n",
        "\n",
        "    p_lbls = glob(os.path.join(path_personal, \"train\", \"labels\", \"*\"))\n",
        "    for lbl in p_lbls:\n",
        "        shutil.copy(lbl, os.path.join(FINAL_DIR, \"train\", \"labels\", os.path.basename(lbl)))\n",
        "    print(f\"   âœ… Added {len(p_imgs)} personal images.\")\n"
      ],
      "metadata": {
        "id": "LZUVONTFeAZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8501a51a-3c30-47e9-98f5-26621779dd13"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âš™ï¸ Processing Data...\n",
            "   â„¹ï¸ Mapping '0' (ID 0) -> Target ID 0\n",
            "   âœ… Added 200 images.\n",
            "   â„¹ï¸ Mapping 'eating-person' (ID 0) -> Target ID 1\n",
            "   âœ… Added 200 images.\n",
            "   â„¹ï¸ Mapping 'drink-water' (ID 0) -> Target ID 2\n",
            "   âœ… Added 200 images.\n",
            "   â„¹ï¸ Adding Personal Data (All)...\n",
            "   âœ… Added 119 personal images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "# Path to merged training labels\n",
        "train_labels_path = f\"{FINAL_DIR}/train/labels\"\n",
        "\n",
        "# Initialize counters\n",
        "class_counts = {0: 0, 1: 0, 2: 0}  # 0=phone, 1=food, 2=drink\n",
        "\n",
        "# Loop over all label files\n",
        "for lbl_file in glob(f\"{train_labels_path}/*.txt\"):\n",
        "    with open(lbl_file) as f:\n",
        "        for line in f:\n",
        "            cls = int(line.split()[0])\n",
        "            if cls in class_counts:\n",
        "                class_counts[cls] += 1\n",
        "\n",
        "# Print results\n",
        "print(\"ğŸ“Š Total labels per class:\")\n",
        "for cls_id, count in class_counts.items():\n",
        "    name = ['phone', 'food', 'drink'][cls_id]\n",
        "    print(f\"  {name} ({cls_id}): {count} annotations\")\n",
        "\n",
        "# Optional: count unique images per class\n",
        "images_per_class = {0: set(), 1: set(), 2: set()}\n",
        "for lbl_file in glob(f\"{train_labels_path}/*.txt\"):\n",
        "    base = os.path.splitext(os.path.basename(lbl_file))[0]\n",
        "    with open(lbl_file) as f:\n",
        "        for line in f:\n",
        "            cls = int(line.split()[0])\n",
        "            if cls in images_per_class:\n",
        "                images_per_class[cls].add(base)\n",
        "\n",
        "print(\"\\nğŸ“Š Total unique images per class:\")\n",
        "for cls_id, imgs in images_per_class.items():\n",
        "    name = ['phone', 'food', 'drink'][cls_id]\n",
        "    print(f\"  {name} ({cls_id}): {len(imgs)} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i83Bm3ePmFBj",
        "outputId": "031d9717-a7df-4bfc-c665-06b4d2a25cb0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Total labels per class:\n",
            "  phone (0): 592 annotations\n",
            "  food (1): 601 annotations\n",
            "  drink (2): 586 annotations\n",
            "\n",
            "ğŸ“Š Total unique images per class:\n",
            "  phone (0): 445 images\n",
            "  food (1): 573 images\n",
            "  drink (2): 580 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6nkVBpIdbzn",
        "outputId": "d286a0be-5c37-4774-ae2c-3aa40ef67055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Training images: 1313, Validation images: 329\n",
            "\n",
            "ğŸ”¥ Starting Training...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 333.9MB/s 0.0s\n",
            "Ultralytics 8.4.9 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/merged_dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=behavior_model_final, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=5, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/behavior_model_final, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 119.5MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, 16, None, [64, 128, 256]] \n",
            "Model summary: 130 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.3MB 322.7MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1115.8Â±303.9 MB/s, size: 50.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/merged_dataset/train/labels... 1313 images, 0 backgrounds, 38 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1313/1313 2.1Kit/s 0.6s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894899_jpg.rf.cacaf54b43ae37598a6cf600ddb80661.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894911_jpg.rf.015da6bf0336d703ade76e024c955934.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894913_jpg.rf.72580a9f41b08f49d098c4a4d3aa34e6.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894917_jpg.rf.f145c0a4f71f872911d2e4f3e9564599.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894918_jpg.rf.9e9732c31fdb24c22bde5f7d8bd9dc60.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894919_jpg.rf.e67645500389d4136f711755d7b7d062.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894920_jpg.rf.3e3005cf6710dc2adbb25c22ea3cb5c4.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894921_jpg.rf.317199b4bda8acf83360abc520da53ac.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894924_jpg.rf.49ba017691f994d26c026b0bd9d374ab.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894925_jpg.rf.ea68838f922198b91fd4ef334d9993ec.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894926_jpg.rf.a881ad3dd429a1405d42367f1cd1bf49.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894927_jpg.rf.3e8189294f97238258cf0c6e8052463c.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894930_jpg.rf.1ec1012538c80afdba5835902ab04011.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894934_jpg.rf.98d39e07830a9ec34e39f38ce1efed56.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894937_jpg.rf.0a0bd6f78bc9c2820ee12ac389beff86.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894939_jpg.rf.5af8a87a1b1d44e4b3a17f18d072b2fb.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894940_jpg.rf.53bd4857f0bab1303604a67c2077f89c.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894941_jpg.rf.efb4666584b9457866a35651b407a42c.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894942_jpg.rf.39a2ec05898e9a104b0f4c0d534ab0b6.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894944_jpg.rf.3185eca3b7a7748c57a6ad99b4c48dc1.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894949_jpg.rf.af37108cfb61d0b811ce027766cd2d8a.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894952_jpg.rf.e0b957fb3f29fd7922010091c99ef5c8.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894955_jpg.rf.1a8703892fb551ae270ea3f0a01e6e10.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894957_jpg.rf.2d346d5fd3e9164781617bf91898eb81.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894958_jpg.rf.47f1698223023ea32813004389206d63.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894959_jpg.rf.0ee4e6683636cc34a9014fe380613441.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894960_jpg.rf.cc66007380ff07c04bd53a6eeb9d60fe.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894961_jpg.rf.2114165d9faf3cecfe162e56d285325a.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894963_jpg.rf.c92c2031a31a1f070d3d94ff7549b209.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894964_jpg.rf.9ad66972d92795d2d66f09ad5da36832.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894965_jpg.rf.2adf0afc1c9fbb2bed92ea405ac20c0a.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894966_jpg.rf.f9775f62032bf35fec3dae58cf65f6e8.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894967_jpg.rf.1673975ceb53296c6ed25d4ed68988e8.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894969_jpg.rf.7317ec714aae0eb2b19365b5ab756748.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894970_jpg.rf.47ffaddacdb67eba56245036195aa3e3.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894971_jpg.rf.75d5cf441f5ff82e593477fca662bc8b.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894972_jpg.rf.c31790a7019a4364a86f798af7866a52.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/merged_dataset/train/images/phone_1769894973_jpg.rf.2649ed04752e5b84a82bbb351eaf7561.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/merged_dataset/train/labels.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 96, len(boxes) = 1417. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 415.5Â±100.4 MB/s, size: 48.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/merged_dataset/valid/labels... 329 images, 0 backgrounds, 6 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 329/329 1.2Kit/s 0.3s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/merged_dataset/valid/images/phone_1769894923_jpg.rf.a7558a899ac2f4c8861be82a0a6e7302.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/merged_dataset/valid/images/phone_1769894938_jpg.rf.4c32d41b0142a07b63f4c39b203cae28.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/merged_dataset/valid/images/phone_1769894943_jpg.rf.a0d3ab1c89cff63be02c478b5f2a54e4.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/merged_dataset/valid/images/phone_1769894950_jpg.rf.34c4a2d73e28d0c0fea065ca25e29e00.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/merged_dataset/valid/images/phone_1769894962_jpg.rf.f578ba5f51b7fed99e633f76b3e91ab1.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/merged_dataset/valid/images/phone_1769894968_jpg.rf.544dcc12baa6c5576c804dc081374398.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 3. Possible class labels are 0-2\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/merged_dataset/valid/labels.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 15, len(boxes) = 362. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "Plotting labels to /content/runs/detect/behavior_model_final/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/behavior_model_final\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50      2.04G      1.342      2.716      1.703         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.3it/s 24.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.8it/s 3.9s\n",
            "                   all        323        362      0.634      0.387      0.453      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50      2.52G      1.386      2.214      1.734         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.4it/s 3.3s\n",
            "                   all        323        362      0.499      0.438      0.425      0.175\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50      2.54G      1.406      2.021      1.744         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.3it/s 3.3s\n",
            "                   all        323        362      0.402      0.523      0.407      0.175\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50      2.55G      1.454      2.019      1.774         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.3it/s 3.4s\n",
            "                   all        323        362      0.329      0.456      0.319      0.141\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50      2.57G      1.417      1.873      1.737         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.4it/s 3.2s\n",
            "                   all        323        362       0.59      0.537      0.532      0.262\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50      2.59G      1.366      1.732      1.686         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.2it/s 3.4s\n",
            "                   all        323        362        0.6      0.578      0.597      0.303\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50      2.61G       1.33      1.628      1.652         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.8it/s 3.9s\n",
            "                   all        323        362      0.667       0.65      0.717      0.331\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50      2.62G      1.318      1.545       1.63         23        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.5it/s 4.3s\n",
            "                   all        323        362      0.716      0.606      0.694      0.357\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50      2.64G      1.256      1.411       1.59         21        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.5it/s 4.4s\n",
            "                   all        323        362      0.574      0.619      0.651      0.316\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50      2.66G      1.237      1.368      1.564         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.4it/s 4.6s\n",
            "                   all        323        362      0.862      0.715      0.816      0.463\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50      2.67G      1.246      1.405      1.574         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.7it/s 4.0s\n",
            "                   all        323        362      0.568      0.564      0.601      0.301\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50      2.69G      1.218      1.349      1.567         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.7it/s 4.1s\n",
            "                   all        323        362      0.841      0.731      0.824      0.489\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50      2.71G      1.172      1.251      1.504         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 20.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.1it/s 3.6s\n",
            "                   all        323        362      0.792      0.656      0.798       0.44\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50      2.72G      1.184      1.231      1.503         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.5it/s 3.2s\n",
            "                   all        323        362      0.821      0.808      0.863      0.529\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50      2.74G      1.179      1.242      1.511         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.3it/s 3.3s\n",
            "                   all        323        362      0.671      0.622      0.746      0.403\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50      2.76G      1.132      1.168      1.473         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.1it/s 3.6s\n",
            "                   all        323        362      0.831      0.804      0.897      0.552\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50      2.78G      1.096      1.116      1.452         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.8it/s 2.9s\n",
            "                   all        323        362      0.841       0.82      0.876      0.552\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50      2.79G      1.094      1.125      1.438         33        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.8it/s 2.9s\n",
            "                   all        323        362      0.874      0.773       0.89      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50      2.81G      1.092      1.093      1.437         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.5it/s 3.1s\n",
            "                   all        323        362      0.862      0.772      0.877      0.545\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50      2.83G      1.061      1.061      1.427         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.7it/s 3.0s\n",
            "                   all        323        362      0.883      0.779      0.904      0.557\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50      2.85G      1.025      1.011      1.392         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.2it/s 3.4s\n",
            "                   all        323        362      0.863      0.817      0.913      0.565\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50      2.86G      1.029      1.013      1.408         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.9it/s 3.8s\n",
            "                   all        323        362      0.846      0.844      0.922      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50      2.88G      1.019     0.9788      1.391         23        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.0it/s 20.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.6it/s 4.3s\n",
            "                   all        323        362      0.897      0.834       0.92      0.597\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50       2.9G      1.004     0.9815      1.381         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.0it/s 20.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.7it/s 4.1s\n",
            "                   all        323        362      0.906      0.829      0.928      0.616\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50      2.91G     0.9717     0.9275      1.358         21        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.6it/s 3.1s\n",
            "                   all        323        362      0.882      0.848      0.929      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50      2.93G     0.9762     0.9138      1.356         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.3it/s 3.3s\n",
            "                   all        323        362      0.854      0.863      0.928      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50      2.95G     0.9387     0.8715      1.332         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.4it/s 3.3s\n",
            "                   all        323        362      0.916      0.833      0.934      0.606\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50      2.96G      0.947     0.8728      1.329         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.8it/s 2.9s\n",
            "                   all        323        362      0.921      0.857      0.946      0.642\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50      2.98G     0.9115     0.8631      1.317         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.7it/s 21.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.8it/s 2.9s\n",
            "                   all        323        362      0.939       0.79      0.923      0.606\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50         3G     0.9065     0.8321      1.312         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.6it/s 3.0s\n",
            "                   all        323        362      0.959      0.815      0.937       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50      3.02G     0.9108     0.8469       1.31         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.5it/s 3.2s\n",
            "                   all        323        362      0.947      0.851      0.945      0.652\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50      3.03G     0.8804     0.8226      1.295         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.4it/s 3.2s\n",
            "                   all        323        362      0.956      0.826      0.947      0.665\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50      3.05G     0.8856     0.8078      1.287         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.0it/s 20.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.6it/s 4.2s\n",
            "                   all        323        362      0.872      0.913      0.955      0.663\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50      3.07G     0.8708     0.7917      1.286         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.8it/s 3.9s\n",
            "                   all        323        362      0.925      0.891      0.951      0.679\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50      3.08G     0.8498     0.7486       1.26         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.0it/s 20.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.9it/s 3.7s\n",
            "                   all        323        362      0.956      0.847      0.959      0.675\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50       3.1G     0.8422     0.7436      1.268         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.3it/s 3.3s\n",
            "                   all        323        362      0.916      0.893      0.952      0.676\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50      3.12G       0.82     0.7276      1.245         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.7it/s 3.0s\n",
            "                   all        323        362      0.874      0.935      0.954       0.68\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50      3.13G     0.8127     0.7326      1.239         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.4it/s 3.2s\n",
            "                   all        323        362      0.909      0.901      0.948      0.684\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/50      3.15G     0.8118     0.7237      1.249         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 20.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.3it/s 3.3s\n",
            "                   all        323        362      0.927      0.886      0.957      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/50      3.17G      0.803     0.7218      1.226         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.8it/s 21.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.4it/s 3.2s\n",
            "                   all        323        362       0.92      0.912       0.96      0.699\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      41/50      3.19G     0.7195     0.5576       1.23         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.6it/s 22.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.9it/s 2.8s\n",
            "                   all        323        362      0.953       0.87      0.957       0.69\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      42/50       3.2G      0.703     0.5209      1.195         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.1it/s 19.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.4it/s 3.3s\n",
            "                   all        323        362      0.951      0.866      0.954      0.694\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      43/50      3.22G     0.6752     0.4946      1.186         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.3it/s 18.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.6it/s 4.3s\n",
            "                   all        323        362      0.952      0.897      0.963       0.72\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      44/50      3.24G     0.6692     0.4798      1.176         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.1it/s 19.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 4.0it/s 2.7s\n",
            "                   all        323        362      0.975      0.855      0.964      0.725\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      45/50      3.26G     0.6398     0.4729      1.151         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.1it/s 19.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.6it/s 3.1s\n",
            "                   all        323        362      0.873      0.959      0.962      0.736\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      46/50      3.27G     0.6259     0.4567      1.149         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 3.9it/s 20.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.7it/s 3.0s\n",
            "                   all        323        362      0.914      0.921      0.964       0.74\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      47/50      3.29G     0.5989     0.4419      1.138         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.0it/s 19.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 3.2it/s 3.5s\n",
            "                   all        323        362      0.903      0.933      0.966      0.743\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      48/50      3.31G     0.5858     0.4278      1.119         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.1it/s 19.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.7it/s 4.0s\n",
            "                   all        323        362      0.934      0.906      0.968      0.743\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      49/50      3.32G     0.5827     0.4283      1.117         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.1it/s 19.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 4.0it/s 2.8s\n",
            "                   all        323        362       0.91      0.923      0.968      0.754\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      50/50      3.34G     0.5694     0.4237      1.099         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 80/80 4.0it/s 20.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 4.0it/s 2.8s\n",
            "                   all        323        362      0.944      0.894      0.967      0.762\n",
            "\n",
            "50 epochs completed in 0.344 hours.\n",
            "Optimizer stripped from /content/runs/detect/behavior_model_final/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from /content/runs/detect/behavior_model_final/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating /content/runs/detect/behavior_model_final/weights/best.pt...\n",
            "Ultralytics 8.4.9 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 73 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 2.3it/s 4.9s\n",
            "                   all        323        362      0.939      0.898      0.967      0.762\n",
            "                 phone         95        127      0.974      0.896      0.947      0.802\n",
            "                  food        109        115      0.898      0.917      0.977      0.737\n",
            "                 drink        119        120      0.946       0.88      0.977      0.745\n",
            "Speed: 0.2ms preprocess, 3.2ms inference, 0.0ms loss, 4.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/behavior_model_final\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 4. TRAIN (UPDATED)\n",
        "# ==========================================\n",
        "import os, shutil, random\n",
        "from glob import glob\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# -------------------------------\n",
        "# 1ï¸âƒ£ Prepare train/val split (80/20)\n",
        "# -------------------------------\n",
        "train_dir = os.path.join(FINAL_DIR, 'train')\n",
        "val_dir = os.path.join(FINAL_DIR, 'valid')\n",
        "\n",
        "# Create valid folders\n",
        "os.makedirs(os.path.join(val_dir, 'images'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'labels'), exist_ok=True)\n",
        "\n",
        "# Get all training images\n",
        "all_images = glob(os.path.join(train_dir, 'images', '*.jpg'))\n",
        "random.shuffle(all_images)\n",
        "split_idx = int(0.8 * len(all_images))\n",
        "\n",
        "train_imgs = all_images[:split_idx]\n",
        "val_imgs = all_images[split_idx:]\n",
        "\n",
        "# Move validation images and labels\n",
        "for img_path in val_imgs:\n",
        "    base = os.path.basename(img_path)\n",
        "    lbl_path = os.path.join(train_dir, 'labels', base.replace('.jpg', '.txt'))\n",
        "\n",
        "    shutil.move(img_path, os.path.join(val_dir, 'images', base))\n",
        "    if os.path.exists(lbl_path):\n",
        "        shutil.move(lbl_path, os.path.join(val_dir, 'labels', base.replace('.jpg', '.txt')))\n",
        "\n",
        "print(f\"ğŸ“Š Training images: {len(train_imgs)}, Validation images: {len(val_imgs)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2ï¸âƒ£ Create data.yaml\n",
        "# -------------------------------\n",
        "yaml_content = f\"\"\"\n",
        "train: {os.path.join(train_dir, 'images')}\n",
        "val: {os.path.join(val_dir, 'images')}\n",
        "nc: 3\n",
        "names: ['phone', 'food', 'drink']\n",
        "\"\"\"\n",
        "with open(os.path.join(FINAL_DIR, \"data.yaml\"), \"w\") as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"\\nğŸ”¥ Starting Training...\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3ï¸âƒ£ Train YOLOv8 model\n",
        "# -------------------------------\n",
        "model = YOLO('yolov8n.pt')  # use small model for Colab T4\n",
        "\n",
        "results = model.train(\n",
        "    data=os.path.join(FINAL_DIR, \"data.yaml\"),\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    name='behavior_model_final',\n",
        "    patience=5,       # early stop if val loss doesn't improve\n",
        "    save_period=5     # save checkpoints every 5 epochs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DoKuOavddk35"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}